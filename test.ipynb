{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data = \"\"\"\n",
    "riyansh Neema\n",
    "I am an enthusiastic Data Scientist with a strong interest in Natural Language Processing (NLP). My passion lies in continuously learning new technology. I also have experience working as a Technical Writer Engineer. Outside of work, I love traveling, writing technical blogs, playing badminton, and swimming.\n",
    "neemapriyanshrd786@gmail.com +91-9993165137 priyanshneema.carrd.co/\n",
    "linkedin.com/in/priyansh-neema-3899a0175 github.com/Priyansh-jsk neemapriyanshrd786.medium.com\n",
    "EDUCATION\n",
    "B.Tech Computer Science\n",
    "Shri Vaishnav Vidyapeeth Vishwavidyalaya\n",
    "08/2018 - 07/2022, Indore\n",
    "WORK EXPERIENCE\n",
    "AI/ML Engineer Nomyx\n",
    "09/2023 – Present Remote\n",
    "Achievements/Tasks - Work on LLM models.\n",
    "AI/ML Engineer\n",
    "Coto: A Web3 social community app\n",
    "04/2023 - 07/2023, Remote\n",
    "Achievements/Tasks\n",
    "SKILLS\n",
    "C/C++, Data Structure, Core Java, Python, AI/Machine Learning, NLP, Image Processing/ Image Classification, LLM, OCI\n",
    "Cloud Tech: AWS, AzureOCR, Docker, Git & GitHub.\n",
    "Data Management: MySQL, MongoDB.\n",
    "Platforms: Linux, Window\n",
    "Web Technologies: HTML5, CSS.\n",
    "Developing AI/ML models and algorithms to solve real-world problems. This involves tasks such as data preprocessing, feature engineering. Frameworks and libraries used- NLTK, TensorFlow, scikit-learn.\n",
    "Work on leveraging AzureOCR and EasyOCR, which are optical character recognition (OCR) tools.\n",
    "Create and manage a MongoDB pipeline for various operations. Example- How many like, comment share on post.\n",
    "Work with Apache Airflow, which is a platform for orchestrating complex workflows and scheduling tasks.\n",
    "Work on Docker a containerization platform, to package and deploy applications and services.\n",
    "AI/ML Engineer\n",
    "Avyay Solutions is spinoff company of Krypt\n",
    "07/2022 - 04/2023, Bangalore\n",
    "Achievements/Tasks\n",
    "Developed Rules-based/NLP Process Flows. Work on ML large dataset for multi-model testing & evaluation.\n",
    "Model used- VGG16, Supervised Machine learning.\n",
    "Work on NLP, Text pre-processing, Image processing/ Image Classification. NLP pipeline- stemming and lemmatization, Bag of words, TF-IDF.\n",
    "Software Developer Intern\n",
    "HotWax Systems\n",
    "01/2022 - 06/2022, Indore\n",
    "Achievements/Tasks\n",
    "Work on Core java JSP + Servlet, MySQL. Work on Apache OFBiz framework\n",
    "LEADERSHIP & ACTIVITIES\n",
    "Technical Content Writer Intern at intervue.io\n",
    "GDG Cloud Indore: Volunteer (09/2019 - 11/2019)\n",
    "Participated in Void Hacks 2.0, an online Hackathon conducted by S.V.V.V.\n",
    "Participated in Data Science Quiz- Data Grandmasters organized by Cliff.ai\n",
    "Attend Nullcon Security Conference\n",
    "CERTIFICATES\n",
    "OpenCV for Beginners (12/2022 - 01/2023) CoRise SQL Crash Course (10/2022 - 11/2022) HackerRank: Problem Solving (03/2020 - 05/2020)\n",
    "Completed ChatGPT Prompt engineer course by DeepLearning.ai.\n",
    "PROJECTS\n",
    "MyTutor E Learning Portal ● WhatsApp Chat Analyzer\n",
    "GoLang (CRUD operation) ● Student Management\n",
    "System using JSP Servlet ● Face emotion analysis\n",
    "Image processing/Image classification ● AzureOCR\n",
    "ChatBot using LLM.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "JD = \"\"\"\n",
    "Job Description: DevOps Azure AI Engineer:   Role Overview\n",
    "      We're on the hunt for an Azure Open AI DevOps Engineer to join our dynamic team. As a DevOps Engineer specialising in AI solutions, you will play a crucial role in building, testing, deploying, and maintaining cutting-edge AI applications on the Azure platform. You'll work alongside our AI Engineers, Business Analysts, Testers, Product Owners, Architects, and Scrum Masters to deliver game-changing solutions for our clients.\n",
    "      Your ability to communicate effectively with these stakeholders and bridge the gap between development and operations will be crucial for the success of our AI projects.\n",
    "      Key Responsibilities\n",
    "      * Develop and implement AI solutions using the Azure stack, including but not limited to Azure Open AI and Azure AI Studio, implement and manage Azure-based infrastructure for AI workloads\n",
    "      * Design, implement, and maintain CI/CD pipelines for AI solutions using Azure DevOps or GitHub Actions\n",
    "      * Collaborate with cross-functional teams to ensure seamless integration of AI solutions, optimise deployments, performance, security, and scalability of AI solutions\n",
    "      * Stay up-to-date with the latest trends and technologies in DevOps and AI and industry best practices\n",
    "      * Implement best practices for DevOps and continuously improve our development and deployment processes\n",
    "      * Develop and manage our Azure infrastructure using Infrastructure as Code (IaC), preferably Terraform\n",
    "      * Containerise AI applications using Docker\n",
    "      * Implement and manage monitoring and logging solutions using Azure Monitor and Application Insights\n",
    "      * Troubleshoot and resolve issues in development, testing, and production environments\n",
    "      * Interface with internal stakeholders and clients to understand infrastructure requirements\n",
    "      Required Skills and Qualifications\n",
    "      * Bachelor's degree in Computer Science, Engineering, or an AI or ML related field\n",
    "      * 2+ years of experience in DevOps or similar roles using DevOps practices and tools\n",
    "      * Strong experience with Azure cloud services, particularly in Azure Open AI and AI Studio\n",
    "      * Familiarity with Infrastructure as Code concepts and tools (preferably Terraform)\n",
    "      * Familiarity with Identity, RBAC and Cloud Security\n",
    "      * Strong proficiency in Python programming\n",
    "      * Experience with version control systems (GitHub)\n",
    "      * Experience with CI/CD pipelines, preferably Azure DevOps or GitHub Actions\n",
    "      * Knowledge of AI/ML concepts and frameworks (e.g., TensorFlow, PyTorch, etc.)\n",
    "      * Proficiency in containerisation and orchestration (Kubernetes, AKS, etc.)\n",
    "      * Extensive experience with Azure cloud platform and services, particularly:\n",
    "        * Azure OpenAI Service\n",
    "        * Azure Cognitive Services\n",
    "        * Azure databases (e.g., CosmosDB, Azure SQL Database)\n",
    "        * Azure Kubernetes Service (AKS)\n",
    "        * Azure AI Studio\n",
    "      * Experience working with Microsoft Bot Framework, Power Platform, Sharepoint\n",
    "      * Experience with API development and management\n",
    "      * Familiarity with Agile methodologies, particularly Scrum\n",
    "      * Excellent problem-solving skills, analytical skills and communication skills\n",
    "      Nice to Have\n",
    "      * At least one of the Azure DevOps Engineer certifications (AZ-900, AZ-104, AZ-204, AZ-400)\n",
    "      * At least one of the Azure AI certifications (e.g., AI-900 or AI-102)\n",
    "      * Scrum Agile certification (e.g., Professional Scrum Master I) or demonstrable experience working in Scrum teams\n",
    "      Preferred Qualifications\n",
    "      * Experience with AWS stack, allowing for multi-cloud comparisons and migrations\n",
    "      * Experience using Notion for project management and collaboration\n",
    "      * Additional Azure certifications (e.g., Azure Solutions Architect Expert / AZ-305)\n",
    "      * Contribution to open-source AI projects\n",
    "      * Hands-on experience with Azure OpenAI Service and Azure AI Studio, including model deployment and fine-tuning\n",
    "      * Familiarity with responsible AI practices and ethical considerations in AI development\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baml_client.sync_client import b\n",
    "from baml_client.types import Resume, Evaluation\n",
    "\n",
    "def ParseResume(raw_resume: str) -> Resume: \n",
    "  # BAML's internal parser guarantees ExtractResume\n",
    "  # to be always return a Resume type\n",
    "  response = b.ExtractResume(raw_resume)\n",
    "  print(response)\n",
    "  return response\n",
    "\n",
    "def EvaluateCandidate(CANDIDATE_RESUME: str, JD: str, github: str) -> Evaluation:\n",
    "\n",
    "  response = b.CandidateEvaluation(CANDIDATE_RESUME,JD,github)\n",
    "  print(response)\n",
    "  return response\n",
    "\n",
    "def FinalEvaluateCandidate(CANDIDATE_RESUME: str, JD: str, \n",
    "                github: str, CallSummaryText:str, evaluation:str) -> Evaluation:\n",
    "\n",
    "  response = b.FinalEvaluationOfCandidate(CANDIDATE_RESUME,JD,github,CallSummaryText,evaluation)\n",
    "  print(response)\n",
    "  return response\n",
    "\n",
    "def FixSummary(callsummary: str, name: str) -> Evaluation:\n",
    "\n",
    "  response = b.FixCallSummary(callsummary ,name)\n",
    "  print(response)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def json_to_csv(json_data_list: List[Dict[str, Any]], output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Convert the given list of JSON data into a structured CSV file in horizontal format.\n",
    "    \n",
    "    :param json_data_list: A list of dictionaries, each containing a candidate's JSON data\n",
    "    :param output_file: The name of the output CSV file\n",
    "    \"\"\"\n",
    "    # Define the fields we want to include in the CSV\n",
    "    fields = [\n",
    "        \"full_name\", \"email\", \"phone\", \"linkedin\", \"website\", \"github\", \"summary\",\n",
    "        \"work_experience\", \"education\", \"skills\", \"projects\", \"certifications\", \"interests\"\n",
    "    ]\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for candidate_data in json_data_list:\n",
    "            row = {}\n",
    "            contact_info = candidate_data.get(\"contact_info\", {})\n",
    "            \n",
    "            # Personal Information\n",
    "            for field in [\"full_name\", \"email\", \"phone\", \"linkedin\", \"website\", \"github\"]:\n",
    "                row[field] = contact_info.get(field, \"\")\n",
    "            \n",
    "            # Summary\n",
    "            row[\"summary\"] = candidate_data.get(\"summary\", \"\")\n",
    "            \n",
    "            # Work Experience\n",
    "            work_exp = candidate_data.get(\"work_experience\", [])\n",
    "            row[\"work_experience\"] = json.dumps(work_exp)  # Store as JSON string\n",
    "            \n",
    "            # Education\n",
    "            education = candidate_data.get(\"education\", [])\n",
    "            row[\"education\"] = json.dumps(education)  # Store as JSON string\n",
    "            \n",
    "            # Skills\n",
    "            skills = candidate_data.get(\"skills\", [])\n",
    "            row[\"skills\"] = json.dumps(skills)  # Store as JSON string\n",
    "            \n",
    "            # Projects\n",
    "            projects = candidate_data.get(\"projects\", [])\n",
    "            row[\"projects\"] = json.dumps(projects)  # Store as JSON string\n",
    "            \n",
    "            # Certifications\n",
    "            certifications = candidate_data.get(\"certifications\", [])\n",
    "            row[\"certifications\"] = json.dumps(certifications)  # Store as JSON string\n",
    "            \n",
    "            # Interests\n",
    "            interests = candidate_data.get(\"interests\", [])\n",
    "            row[\"interests\"] = \", \".join(interests) if interests else \"\"\n",
    "            \n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"CSV file '{output_file}' has been created successfully in horizontal format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact_info=ContactInfo(full_name='Riyansh Neema', email='neemapriyanshrd786@gmail.com', phone='+91-9993165137', address=None, linkedin='linkedin.com/in/priyansh-neema-3899a0175', website='priyanshneema.carrd.co/', github='github.com/Priyansh-jsk') summary='I am an enthusiastic Data Scientist with a strong interest in Natural Language Processing (NLP). My passion lies in continuously learning new technology. I also have experience working as a Technical Writer Engineer. Outside of work, I love traveling, writing technical blogs, playing badminton, and swimming.' typeOfDevelopment=[<TypeofDevelopment.AID: 'AID'>] work_experience=[Experience(company='Nomyx', position='AI/ML Engineer', start_date='2023-09', end_date=None, location='Remote', is_current=True, responsibilities=['Work on LLM models.'], achievements=None), Experience(company='Coto: A Web3 social community app', position='AI/ML Engineer', start_date='2023-04', end_date='2023-07', location='Remote', is_current=False, responsibilities=['Developing AI/ML models', 'data preprocessing', 'feature engineering', 'Work on leveraging AzureOCR and EasyOCR', 'Create and manage a MongoDB pipeline', 'Work with Apache Airflow', 'Work on Docker.'], achievements=None), Experience(company='Avyay Solutions', position='AI/ML Engineer', start_date='2022-07', end_date='2023-04', location='Bangalore', is_current=False, responsibilities=['Developed Rules-based/NLP Process Flows.', 'Work on ML large dataset', 'Model used- VGG16, Supervised Machine learning', 'Work on NLP, Text pre-processing, Image processing/Image Classification', 'NLP pipeline- stemming and lemmatization, Bag of words, TF-IDF.'], achievements=None), Experience(company='HotWax Systems', position='Software Developer Intern', start_date='2022-01', end_date='2022-06', location='Indore', is_current=False, responsibilities=['Work on Core java JSP + Servlet', 'Work on Apache OFBiz framework'], achievements=None)] education=[Education(institution='Shri Vaishnav Vidyapeeth Vishwavidyalaya', degree='B.Tech', field_of_study='Computer Science', graduation_date='2022-07', gpa=None, honors=None)] skills=[Skill(name='C/C++', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='Data Structure', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='Core Java', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='Python', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='AI/Machine Learning', level=<SkillLevel.Expert: 'Expert'>), Skill(name='NLP', level=<SkillLevel.Expert: 'Expert'>), Skill(name='Image Processing/Image Classification', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='LLM', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='OCI', level=<SkillLevel.Intermediate: 'Intermediate'>), Skill(name='AWS', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='AzureOCR', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='Docker', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='Git & GitHub', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='MySQL', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='MongoDB', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='Linux', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='Windows', level=<SkillLevel.Advanced: 'Advanced'>), Skill(name='HTML5', level=<SkillLevel.Intermediate: 'Intermediate'>), Skill(name='CSS', level=<SkillLevel.Intermediate: 'Intermediate'>)] projects=[Project(project=Projects(name='WhatsApp Chat Analyzer', description='Analyzes WhatsApp chat data.', technologies='Python, NLP', category=<TypeofDevelopment.AID: 'AID'>, url=None))] certifications=[Certification(name='Problem Solving', issuing_organization='HackerRank', date_obtained='2020-03-01', expiry_date=None, link=None)] languages=None interests=['traveling', 'writing technical blogs', 'playing badminton', 'swimming']\n",
      "OverallScore=6 Strengths=['Strong AI/ML background, particularly in NLP and image processing', 'Experience with Docker for containerization', 'Proficiency in Python programming', 'Exposure to Azure services'] Weaknesses=['No explicit experience with Azure AI Studio and Terraform', 'Lack of experience with Kubernetes or AKS for orchestration', 'Missing specific Azure DevOps and AI certifications'] Explanation=['The candidate has a strong foundation in AI/ML and relevant skills in Docker and Python.', 'There is some exposure to Azure services, but critical areas like Azure AI Studio, Terraform and Kubernetes are missing.', 'Active GitHub engagement is a plus, showcasing learning and application of skills.'] Recommendation='The candidate needs further evaluation for some critical aspects but showcases potential. They might be a good fit for a less specialized role or with some additional training.' ApplicationStatus=<ApplicationCategory.WorthHavingALook: 'WorthHavingALook'> Questions=['Can you describe your experience with Azure AI Studio and any specific projects you have used it for?', 'Do you have any experience with Infrastructure as Code tools such as Terraform and can you give an example?', 'Have you worked with Kubernetes or any container orchestration tools, and if so, could you describe a project where you used them?']\n"
     ]
    }
   ],
   "source": [
    "from backend.extract_github import extract_github_data\n",
    "\n",
    "output = ParseResume(resume_data)\n",
    "json_data = json.loads(output.model_dump_json())\n",
    "if json_data['contact_info']['github'] != '':\n",
    "    github_profile = json_data['contact_info']['github']\n",
    "    json_github = extract_github_data(github_profile)\n",
    "else:\n",
    "    json_github = {}\n",
    "evaluation_output=EvaluateCandidate(resume_data, JD, str(json_github)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OverallScore': 6,\n",
       " 'Strengths': ['Strong AI/ML background, particularly in NLP and image processing',\n",
       "  'Experience with Docker for containerization',\n",
       "  'Proficiency in Python programming',\n",
       "  'Exposure to Azure services'],\n",
       " 'Weaknesses': ['No explicit experience with Azure AI Studio and Terraform',\n",
       "  'Lack of experience with Kubernetes or AKS for orchestration',\n",
       "  'Missing specific Azure DevOps and AI certifications'],\n",
       " 'Explanation': ['The candidate has a strong foundation in AI/ML and relevant skills in Docker and Python.',\n",
       "  'There is some exposure to Azure services, but critical areas like Azure AI Studio, Terraform and Kubernetes are missing.',\n",
       "  'Active GitHub engagement is a plus, showcasing learning and application of skills.'],\n",
       " 'Recommendation': 'The candidate needs further evaluation for some critical aspects but showcases potential. They might be a good fit for a less specialized role or with some additional training.',\n",
       " 'ApplicationStatus': 'WorthHavingALook',\n",
       " 'Questions': ['Can you describe your experience with Azure AI Studio and any specific projects you have used it for?',\n",
       "  'Do you have any experience with Infrastructure as Code tools such as Terraform and can you give an example?',\n",
       "  'Have you worked with Kubernetes or any container orchestration tools, and if so, could you describe a project where you used them?']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluationjson = json.loads(evaluation_output.model_dump_json())\n",
    "ApplicationStatus = evaluationjson['ApplicationStatus']\n",
    "if ApplicationStatus == \"WorthHavingALook\":\n",
    "    print()\n",
    "    # trigger call with questions: evaluationjson['Questions'] and name of the applicant: json_data['contact_info']['full_name']\n",
    "else:\n",
    "    print(\"Application Rejected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def json_to_csv(json_data_list: List[Any], output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Convert the given list of JSON data into a structured CSV file in horizontal format.\n",
    "    \n",
    "    :param json_data_list: A list containing candidate JSON data (either dictionaries or JSON strings)\n",
    "    :param output_file: The name of the output CSV file\n",
    "    \"\"\"\n",
    "    # Define the fields we want to include in the CSV\n",
    "    fields = [\n",
    "        \"full_name\", \"email\", \"phone\", \"linkedin\", \"website\", \"github\", \"summary\",\n",
    "        \"work_experience\", \"education\", \"skills\", \"projects\", \"certifications\", \"interests\"\n",
    "    ]\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fields)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for candidate_data in json_data_list:\n",
    "            # If candidate_data is a string (JSON format), parse it to a dictionary\n",
    "            if isinstance(candidate_data, str):\n",
    "                candidate_data = json.loads(candidate_data)\n",
    "\n",
    "            row = {}\n",
    "            contact_info = candidate_data.get(\"contact_info\", {})\n",
    "            \n",
    "            # Personal Information\n",
    "            for field in [\"full_name\", \"email\", \"phone\", \"linkedin\", \"website\", \"github\"]:\n",
    "                row[field] = contact_info.get(field, \"\")\n",
    "            \n",
    "            # Summary\n",
    "            row[\"summary\"] = candidate_data.get(\"summary\", \"\")\n",
    "            \n",
    "            # Work Experience\n",
    "            work_exp = candidate_data.get(\"work_experience\", [])\n",
    "            row[\"work_experience\"] = json.dumps(work_exp)  # Store as JSON string\n",
    "            \n",
    "            # Education\n",
    "            education = candidate_data.get(\"education\", [])\n",
    "            row[\"education\"] = json.dumps(education)  # Store as JSON string\n",
    "            \n",
    "            # Skills\n",
    "            skills = candidate_data.get(\"skills\", [])\n",
    "            row[\"skills\"] = json.dumps(skills)  # Store as JSON string\n",
    "            \n",
    "            # Projects\n",
    "            projects = candidate_data.get(\"projects\", [])\n",
    "            row[\"projects\"] = json.dumps(projects)  # Store as JSON string\n",
    "            \n",
    "            # Certifications\n",
    "            certifications = candidate_data.get(\"certifications\", [])\n",
    "            row[\"certifications\"] = json.dumps(certifications)  # Store as JSON string\n",
    "            \n",
    "            # Interests\n",
    "            interests = candidate_data.get(\"interests\", [])\n",
    "            row[\"interests\"] = \", \".join(interests) if interests else \"\"\n",
    "            \n",
    "            writer.writerow(row)\n",
    "\n",
    "    print(f\"CSV file '{output_file}' has been created successfully in horizontal format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
